apiVersion: apps/v1
kind: Deployment
metadata:
  name: text2text-x
  namespace: vllm-k8s-operator-system  # 保持与原有服务相同命名空间
spec:
  replicas: 1
  selector:
    matchLabels:
      app: text2text-x
  template:
    metadata:
      labels:
        app: text2text-x
    spec:
      containers:
      - name: text2text-x
        image: docker.io/dustynv/ollama:0.6.8-r36.4
        imagePullPolicy: IfNotPresent
        env:
        - name: OLLAMA_KEEP_ALIVE
          value: "-1"
        - name: OLLAMA_NUM_PARALLEL
          value: "8"
        - name: OLLAMA_MAX_LOADED_MODELS
          value: "1"
        ports:
        - containerPort: 11434
        volumeMounts:
          - name: qwen7b-volume
            mountPath: /data/models/ollama  # 挂载路径
            subPath: llama31-8b-ollama
        command: ["/bin/bash", "-c"]
        args:
          - |
            /start_ollama && 
            until curl -s http://localhost:11434/api/tags > /dev/null 2>&1; do 
              echo '等待 ollama 服务启动...'; 
              sleep 2; 
            done && 
            echo 'ollama 服务已就绪，激活模型...' && 
            echo 'exit' | ollama run llama3.1:8b && 
            tail -f /dev/null
      dnsPolicy: ClusterFirst
      nodeName: b410-jetson-1
      restartPolicy: Always
      schedulerName: default-scheduler
      volumes:
          - name: qwen7b-volume
            persistentVolumeClaim:
              claimName: model-pvc  # PVC 名称

---
apiVersion: v1
kind: Service
metadata:
  name: text2text-x-service
  namespace: vllm-k8s-operator-system
spec:
  type: NodePort
  ports:
  - port: 11434
    targetPort: 11434
    nodePort: 32113  # 保持与 Docker 相同的端口映射
  selector:
    app: text2text-x
